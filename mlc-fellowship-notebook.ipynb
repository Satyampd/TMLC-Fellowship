{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-27T16:09:48.441551Z","iopub.execute_input":"2021-10-27T16:09:48.442653Z","iopub.status.idle":"2021-10-27T16:09:48.464512Z","shell.execute_reply.started":"2021-10-27T16:09:48.442535Z","shell.execute_reply":"2021-10-27T16:09:48.463899Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Reading the provided data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(r'/kaggle/input/twitter-sentiment-dataset/Twitter_Data.csv')\ndata.sample(5)\n\ndata.dropna(inplace = True)\n# data_df = data = data[:20000] \ndata_df = data ","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:48.466029Z","iopub.execute_input":"2021-10-27T16:09:48.466273Z","iopub.status.idle":"2021-10-27T16:09:49.337369Z","shell.execute_reply.started":"2021-10-27T16:09:48.466241Z","shell.execute_reply":"2021-10-27T16:09:49.336629Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Lets deep dive into the data","metadata":{}},{"cell_type":"code","source":" # length of the longest string in df\ndata.clean_text.str.len().max()","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:49.338878Z","iopub.execute_input":"2021-10-27T16:09:49.339138Z","iopub.status.idle":"2021-10-27T16:09:49.459381Z","shell.execute_reply.started":"2021-10-27T16:09:49.339103Z","shell.execute_reply":"2021-10-27T16:09:49.458581Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.category.value_counts().plot(kind = 'bar')","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:49.461733Z","iopub.execute_input":"2021-10-27T16:09:49.462032Z","iopub.status.idle":"2021-10-27T16:09:49.684136Z","shell.execute_reply.started":"2021-10-27T16:09:49.461996Z","shell.execute_reply":"2021-10-27T16:09:49.683503Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Observation: We are seeing data is not balanced but not also very implance.","metadata":{}},{"cell_type":"markdown","source":"## What are most frequent words","metadata":{}},{"cell_type":"code","source":"from nltk.probability import FreqDist\nentire_text = \" \".join([str(text) for text in data.clean_text.values.tolist()])\nfdist = FreqDist(entire_text.split())\ntop_ten = fdist.most_common(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:49.685502Z","iopub.execute_input":"2021-10-27T16:09:49.685760Z","iopub.status.idle":"2021-10-27T16:09:53.609593Z","shell.execute_reply.started":"2021-10-27T16:09:49.685726Z","shell.execute_reply":"2021-10-27T16:09:53.608852Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"top_ten","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:53.611533Z","iopub.execute_input":"2021-10-27T16:09:53.611951Z","iopub.status.idle":"2021-10-27T16:09:53.620055Z","shell.execute_reply.started":"2021-10-27T16:09:53.611906Z","shell.execute_reply":"2021-10-27T16:09:53.619330Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Observation and Comments: \n* Most Frequent word is \"Modi\", and others are normal stopwords.\n* We will have to remove the stopwords.\n* We can train model with/without keyword \"Modi\".","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Text cleaning\nimport nltk\nfrom bs4 import BeautifulSoup\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nimport re\nfrom tqdm import tqdm\n\ndef text_cleaning(text):\n    # change the text into lower case.(Note: in case of social media text, it is good to leave them as it is!)\n    text=text.lower()\n    # removing xml tags from tweets\n    text=BeautifulSoup(text, 'lxml').get_text()\n    # removing URLS \n    text=re.sub('https?://[A-Za-z0-9./]+','',text)\n    # removing words with \"@\"\n    text=re.sub(r'@[A-Za-z0-9]+','',text) \n    # removing special characters\n    text= re.sub(r\"\\W+|_\", ' ', text)\n    # tokenization of sentences\n    text= word_tokenize(text)\n    # lemmatize the text using WordNet\n    lm=WordNetLemmatizer()\n    words = [lm.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]   \n    \n    return \" \".join(words)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:53.621133Z","iopub.execute_input":"2021-10-27T16:09:53.621398Z","iopub.status.idle":"2021-10-27T16:09:53.781976Z","shell.execute_reply.started":"2021-10-27T16:09:53.621361Z","shell.execute_reply":"2021-10-27T16:09:53.781217Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# data.clean_text = data['clean_text'].apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:53.783490Z","iopub.execute_input":"2021-10-27T16:09:53.783994Z","iopub.status.idle":"2021-10-27T16:09:53.787590Z","shell.execute_reply.started":"2021-10-27T16:09:53.783955Z","shell.execute_reply":"2021-10-27T16:09:53.786698Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Data Transformation","metadata":{}},{"cell_type":"markdown","source":"So, we can not words directly sent normal text to any ML/DL model, so for that we need some techniques where we convert normal text in some form which is understable by models, to name a few, basics ones are BOW, TDIDF and advance such as word2vec, GloVe, fasttext.\n\n#### TFIDF is quite simple understand, we can understand, it gives imprtance to \n1. Rare words in corpus\n2. Common words in document\n\n#### word2vec is another famous method where is trained with deep learning model.\nTwo types of word2vec based on core training idea:\n1. CBOW: Given context word predict focus word\n2. Skipgram: Given focus word predict context words","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.clean_text, data.category, test_size=0.25, random_state=42)\nprint(X_train.shape, X_test.shape)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(ngram_range=(1, 1),\n    max_df=1.0,\n    min_df=1,)\nX_train_tfidf=tfidf.fit_transform(X_train)\nX_test_tfidf=tfidf.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:09:53.788917Z","iopub.execute_input":"2021-10-27T16:09:53.789600Z","iopub.status.idle":"2021-10-27T16:09:58.073652Z","shell.execute_reply.started":"2021-10-27T16:09:53.789561Z","shell.execute_reply":"2021-10-27T16:09:58.072918Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nmodel=LogisticRegression(solver='liblinear')\nmodel.fit(X_train_tfidf, y_train)\n\ny_pred=model.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))\n\nplot_confusion_matrix(model, X_test_tfidf, y_test) ","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:14:24.156313Z","iopub.execute_input":"2021-10-27T16:14:24.156561Z","iopub.status.idle":"2021-10-27T16:14:31.514243Z","shell.execute_reply.started":"2021-10-27T16:14:24.156532Z","shell.execute_reply":"2021-10-27T16:14:31.513529Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_tfidf, y_train)\n\ny_pred=model.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))\nplot_confusion_matrix(model, X_test_tfidf, y_test) ","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:14:35.266209Z","iopub.execute_input":"2021-10-27T16:14:35.266489Z","iopub.status.idle":"2021-10-27T16:26:16.551376Z","shell.execute_reply.started":"2021-10-27T16:14:35.266459Z","shell.execute_reply":"2021-10-27T16:26:16.550650Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Model: LSTM\n\n\nLSTM is a Deep learning model, it is succesor of RNN, as RNN suffers with Gradiant Explosion and does not work with long sequence of text.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:26:16.555808Z","iopub.execute_input":"2021-10-27T16:26:16.557831Z","iopub.status.idle":"2021-10-27T16:26:21.577276Z","shell.execute_reply.started":"2021-10-27T16:26:16.557791Z","shell.execute_reply":"2021-10-27T16:26:21.576547Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Transforming the data for LSTM model","metadata":{}},{"cell_type":"code","source":"max_features = 20000\ntokenizer = Tokenizer(num_words = max_features, )\ntokenizer.fit_on_texts(data_df['clean_text'].values)\nX = tokenizer.texts_to_sequences(data_df['clean_text'].values)\nX = pad_sequences(X, padding = 'post' ,maxlen=300)\nY = pd.get_dummies(data_df['category']).values\n\nvocab_size = len(tokenizer.word_index)+1","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:26:21.578383Z","iopub.execute_input":"2021-10-27T16:26:21.578640Z","iopub.status.idle":"2021-10-27T16:26:30.241209Z","shell.execute_reply.started":"2021-10-27T16:26:21.578608Z","shell.execute_reply":"2021-10-27T16:26:30.240412Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Spliting the entire data into train and test, it is being divided in 75: 25 ratio.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:26:30.243148Z","iopub.execute_input":"2021-10-27T16:26:30.243398Z","iopub.status.idle":"2021-10-27T16:26:30.336899Z","shell.execute_reply.started":"2021-10-27T16:26:30.243362Z","shell.execute_reply":"2021-10-27T16:26:30.336061Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Defining our model","metadata":{}},{"cell_type":"code","source":"embid_dim = 300   # embid_dim will give the word vector value in 300 dimensions\nlstm_out = 128\n\n# will have total 2 layers\nmodel = keras.Sequential()\nmodel.add(Embedding(max_features, embid_dim, input_length = X.shape[1]))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(3, activation = 'softmax'))     # softmax for final layer\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:26:30.338326Z","iopub.execute_input":"2021-10-27T16:26:30.338773Z","iopub.status.idle":"2021-10-27T16:26:33.589048Z","shell.execute_reply.started":"2021-10-27T16:26:30.338733Z","shell.execute_reply":"2021-10-27T16:26:33.588354Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:26:33.590275Z","iopub.execute_input":"2021-10-27T16:26:33.590968Z","iopub.status.idle":"2021-10-27T16:32:58.160458Z","shell.execute_reply.started":"2021-10-27T16:26:33.590928Z","shell.execute_reply":"2021-10-27T16:32:58.159626Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Predication with Test data","metadata":{}},{"cell_type":"code","source":"y_prob = model.predict(X_test)\ny_classes = y_prob.argmax(axis=-1)","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:32:58.162259Z","iopub.execute_input":"2021-10-27T16:32:58.162554Z","iopub.status.idle":"2021-10-27T16:33:19.207277Z","shell.execute_reply.started":"2021-10-27T16:32:58.162525Z","shell.execute_reply":"2021-10-27T16:33:19.206424Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfor i in range(len(list(y_classes))):\n    if y_classes[i] == 0 :\n        y_classes[i] = -1\n    elif  y_classes[i] == 1 :\n        y_classes[i] = 0\n    else:\n        y_classes[i] = 1\n        \nprint(accuracy_score(y_test, y_classes)) ","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:33:19.208656Z","iopub.execute_input":"2021-10-27T16:33:19.208950Z","iopub.status.idle":"2021-10-27T16:33:19.281890Z","shell.execute_reply.started":"2021-10-27T16:33:19.208915Z","shell.execute_reply":"2021-10-27T16:33:19.281144Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test, y_classes), annot=True, fmt='d' , )","metadata":{"execution":{"iopub.status.busy":"2021-10-27T16:33:19.282852Z","iopub.execute_input":"2021-10-27T16:33:19.283046Z","iopub.status.idle":"2021-10-27T16:33:19.598255Z","shell.execute_reply.started":"2021-10-27T16:33:19.283023Z","shell.execute_reply":"2021-10-27T16:33:19.597538Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}